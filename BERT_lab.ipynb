{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishkeni/Stanford_Artificial_Intelliegnce_NLP_NLU_ML/blob/master/BERT_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bQUIQGANA96I"
      },
      "source": [
        "# BERT Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hivbLURpB3b2"
      },
      "source": [
        "In 2018, models such as [BERT](https://arxiv.org/abs/1810.04805), [Open GPT](https://openai.com/blog/better-language-models/) and [ELMO](https://arxiv.org/abs/1802.05365) were released and performed well on many benchmark NLP tasks with minimal task-specific tuning. The publicly available pretrained models can be used either to extract high quality language features from your text data, or fine-tuned on a specific task (classification, entity recognition, etc.) with your data to produce [state of the art predictions](https://gluebenchmark.com/leaderboard).\n",
        "\n",
        "In this notebook we will use a pre-trained BERT model to **extract features**, namely word embedding vectors, from text data. We will explore different ways of extracting these embeddings and build a simple binary classifier model to compare different embedding approaches.\n",
        "\n",
        "Traditionally, words were converted to their vector representations by either **uniquely indexing (one-hot encoding)** them or using neural network based word embeddings such as **Word2Vec or Fasttext**. BERT offers an advantage over models like Word2Vec because while each word has a fixed representation in Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them. For example, given two sentences:\n",
        "\n",
        "<font color='green'>The man was accused of robbing a bank.</font>\n",
        "\n",
        "<font color='green'>The man went fishing by the bank of the river.</font>\n",
        "\n",
        "Models such as Word2Vec would produce the same word embeddings for the word _\"bank\"_ in both sentences (embeddings remain the same regardless of the context), while in BERT word embeddings for _\"bank\"_ would be different for  both sentences. \n",
        "\n",
        "*Note that this is in no way a detailed or a comprehensive explanation on BERT architecture (or its alternatives). It is a brief introduction to BERT and we explore how pre-trained BERT models can be used to extract word embeddings. We will use these embeddings for a simple classification task where we try to predict if a piece of text is a positive or a negative review.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um30Suwd6uGY",
        "colab_type": "text"
      },
      "source": [
        "#### This notebook has two sections:\n",
        "I. **BERT Embeddings Walk Through**:\n",
        "    Here we explore how to extract embeddings from a pre-trained BERT model.\n",
        "\n",
        "II. **Classification**:\n",
        "    Here we build a base binary classification model (logisitic regression) for a binary classification task. Your task will be to try improving the model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Iy7uS7LPCSWN"
      },
      "source": [
        "## I. BERT Embeddings Walk Through"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4LejUWiiA96K"
      },
      "source": [
        "### Format the input sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lToY1wVAFsyO"
      },
      "source": [
        "BERT expects input sentences to be in a specific format:\n",
        "\n",
        "- Tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
        "- Word piece tokenization\n",
        "- Segment IDs used to distinguish different sentences\n",
        "\n",
        "![bert-input-format](https://i.imgur.com/Z8MXvQG.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX-gX731vef7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the required modules\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yssq0Oi5B4Ui"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "97mPwzYKA96L",
        "colab": {}
      },
      "source": [
        "# We are loading a smaller 'bert-base-uncased' model for this notebook. For more information on pre-trained models, check this: https://github.com/google-research/bert#pre-trained-models\n",
        "import torch\n",
        "from transformers import *\n",
        "\n",
        "# model -> BertModel\n",
        "# tokenizer -> BertTokenizer\n",
        "# model name -> 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xDBSw37DA96U"
      },
      "source": [
        "#### Tokenization\n",
        "\n",
        "In the example below, the word \"ephemeral\" is split into smaller subwords and characters. Hash signs preceding these subwords are the tokenizer's way of denoting that this subword or character is part of a larger word and preceded by another subword. \n",
        "\n",
        "The BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fit our language data. To tokenize a word in this model, the tokenizer first checks if the whole word is in the vocabulary. If not, it tries to break the word into the largest possible subwords contained in the vocabulary, and as a last resort will decompose the word into individual characters. Note that because of this, we don't run into the problem of out of vocabulary words.\n",
        "\n",
        "So, rather than assigning \"ephemeral\" and every other out of vocabulary word to something like 'UNK' (unknown vocabulary token), we split it into subword tokens ['ep', '##hem', '##eral'] that will retain some of the contextual meaning of the original word.\n",
        "\n",
        "\n",
        "(For more information about WordPiece, see the [original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) and further disucssion in Google's [Neural Machine Translation System](https://arxiv.org/pdf/1609.08144.pdf).)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-8ZI17NHA96Q",
        "colab": {}
      },
      "source": [
        "# Using the BERT tokenizer\n",
        "example_text = \"ephemeral\"\n",
        "print(tokenizer.tokenize(example_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lv20c8GVA96V",
        "colab": {}
      },
      "source": [
        "# Adding tokens [CLS] and [SEP] at start and end of a sentence\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "print (marked_text)\n",
        "\n",
        "# Using BERT word piece tokenizer \n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "print (\"\\nAfter tokenization:\\n\",tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5mc_DnkUA96Z"
      },
      "source": [
        "#### Segment IDs\n",
        "\n",
        "BERT is trained on and expects sentence pairs (The [SEP] token is used to separate multiple sentences), using 1s and 0s to distinguish between the two sentences. \n",
        "\n",
        "That is, for each token in \"tokenized_text,\" we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence. \n",
        "\n",
        "If you want to process two sentences, assign each word in the first sentence plus the '[SEP]' token a 0, and all tokens of the second sentence a 1.\n",
        "\n",
        "Essentially you can have multiple sentence pairs as shown below:\n",
        "\n",
        "- **\"<font color='green'>[CLS] The man was accused of robbing a bank. [SEP] The man was seen fishing by the river bank. [SEP]</font>\"**\n",
        "  - tokens in this case would be: ['[CLS]', 'the', 'man', 'was', 'accused', 'of', 'robb', '##ing', 'a', 'bank', '.', '[SEP]', 'the', 'man', 'was', 'seen', 'fishing', 'by', 'the', 'river', 'bank', '.', '[SEP]']\n",
        "  - segment ids in this case would be: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "  \n",
        "\n",
        "or you can have it as two separate sentences as shown below:\n",
        "\n",
        "- **\"<font color='green'>[CLS] The man was accused of robbing a bank. [SEP]</font>\"**\n",
        "  - tokens in this case would be: ['[CLS]', 'the', 'man', 'was', 'accused', 'of', 'robb', '##ing', 'a', 'bank', '.', '[SEP]']\n",
        "  - segment ids in this case would be: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "- **\"<font color='green'>[CLS] The man was seen fishing by the river bank. [SEP]</font>\"**\n",
        "  - tokens in this case would be: ['[CLS]', 'the', 'man', 'was', 'seen', 'fishing', 'by', 'the', 'river', 'bank', '.', '[SEP]']\n",
        "  - segment ids in this case would be: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F6Yqic-gA96b",
        "colab": {}
      },
      "source": [
        "# For our task we're using single sentence per input text\n",
        "segments_ids = [0] * len(tokenized_text)\n",
        "print (segments_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hita8_R2A96e"
      },
      "source": [
        "#### Convert tokens to indices\n",
        "\n",
        "We call the tokenizer to match the tokens against their indices in the tokenizer vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXa99b1UA96f",
        "colab": {}
      },
      "source": [
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print(tup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rEpHYlAZA96i"
      },
      "source": [
        "### Forward pass the input sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJLcUBF5GOyU"
      },
      "source": [
        "1. The pre-trained BERT PyTorch interface expects the data be in torch tensor format so we first convert the segment_ids and tokenized text \n",
        " \n",
        "2. model.eval() puts our model in evaluation mode as opposed to training mode.\n",
        "\n",
        "3. Calling `from_pretrained` will download the pre-trained model specified. When we load the `bert-base-uncased`, we see the definition of the model printed. This particular model has 13 layers (the forward pass of the model includes the outputs of the initial embedding in addition to the encoder states, so we get 13 layers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FkmyKBl1A96j",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states=True)\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ko3otJz5A96n",
        "colab": {}
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    encoded_layers = model(tokens_tensor, token_type_ids=segments_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yR3Xme5fA96r",
        "colab": {}
      },
      "source": [
        "# layers x batch_size x sequence length x hidden state dimension\n",
        "n_layers, batch_size, sequence_length, hidden_state_dim = encoded_layers[-1], encoded_layers[0], encoded_layers[0][0], encoded_layers[0][0][0]\n",
        "len(n_layers), len(batch_size), len(sequence_length), len(hidden_state_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a5ssRnKeA96u"
      },
      "source": [
        "#### BERT output\n",
        "\n",
        "The `encoded_layers` variable in the section above has four dimensions:\n",
        "\n",
        "1. The layer number (13 layers)\n",
        "2. The batch number (1 sentence)\n",
        "3. The word / token number (number of tokens in our sentence)\n",
        "4. Hidden state for that layer (768 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qqNlWUjbA96v",
        "colab": {}
      },
      "source": [
        "# Convert the hidden state embeddings into single token vectors\n",
        "\n",
        "# Will have the shape: [# tokens, # layers, # features]\n",
        "token_embeddings = [] \n",
        "\n",
        "batch_i = 0\n",
        "# For each token in the sentence...\n",
        "for token_i in range(len(tokenized_text)):\n",
        "  \n",
        "    # Holds 12 layers of hidden states for each token \n",
        "    hidden_layers = [] \n",
        "\n",
        "    # For each of the 12 layers...[last layer is excluded. See issue : https://github.com/huggingface/transformers/issues/1332]\n",
        "    for layer_i in range(len(encoded_layers[-1][0:12])):\n",
        "\n",
        "        # Lookup the vector for `token_i` in `layer_i`\n",
        "        vec = encoded_layers[-1][layer_i][batch_i][token_i]\n",
        "\n",
        "        hidden_layers.append(vec)\n",
        "    \n",
        "    token_embeddings.append(hidden_layers)\n",
        "\n",
        "# Sanity check the dimensions:\n",
        "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
        "print (\"Number of layers per token:\", len(token_embeddings[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qI1OBkNjA96y"
      },
      "source": [
        "### Create word and sentence embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ytonVXQWGcei"
      },
      "source": [
        "To extract features we'd like individual vectors for each of our tokens, or even a single vector representation of the whole sentence.\n",
        "\n",
        "In order to get the individual vectors we can combine some of the layer vectors. BERT authors tested different combinations of this by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores.\n",
        "\n",
        "![bert-feature-extraction-contextualized-embeddings](https://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n",
        "\n",
        "Using the last four layers produced the best results on this specific task. Many of the other methods returned close results, and so it is advisable to test different versions depending on the specific application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7HR2ENfjA96z"
      },
      "source": [
        "#### Word vectors\n",
        "\n",
        "Let's create word vectors by **using just the last layer**, to give us a single vector per token. Each of those vectors will have a length of `768`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oLU97qgOA960",
        "colab": {}
      },
      "source": [
        "# Every token vector : [sentence sequence length x 768]\n",
        "token_vecs = [each_token[-1] for each_token in token_embeddings]\n",
        "\n",
        "print('shape of token_vecs: %d x %d' % (len(token_vecs), len(token_vecs[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZDba-lymA964"
      },
      "source": [
        "Another way of creating word vectors is by **summing** together the last four layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKIZ-_ZeA966",
        "colab": {}
      },
      "source": [
        "# Stores the token vectors, with shape [sentence sequence length x 768]\n",
        "token_vecs_sum = []\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qPBbx0KCA96-"
      },
      "source": [
        "#### Sentence vectors\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the last hidden layer of each token, producing a single 768 length vector. \n",
        "<a href=\"https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence#answer-31738627\">Here</a> are different ways of creating sentence vectors from word vectors (the link refers to Word2Vec embeddings, however the approach can be used here as well). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iFwwn25IA96_",
        "colab": {}
      },
      "source": [
        "sentence_embedding = torch.mean(encoded_layers[-1][11], 1)\n",
        "print(\"shape of sentence embedding\", list(sentence_embedding.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BpRnRCl5A97D"
      },
      "source": [
        "### Confirming contextually dependent vectors\n",
        "\n",
        "To confirm that the value of these vectors are in fact contextually dependent, let's take a look at the output from the following sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q1xIrelVA97F",
        "colab": {}
      },
      "source": [
        "print (text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zdjvu57AA97J",
        "colab": {}
      },
      "source": [
        "for i,x in enumerate(tokenized_text):\n",
        "    print(i,x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t6hXhbvkA97M",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"river bank\"\n",
        "different_bank = cosine_similarity(token_vecs_sum[10].reshape(1,-1), token_vecs_sum[19].reshape(1,-1))[0][0]\n",
        "\n",
        "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"bank vault\" \n",
        "same_bank = cosine_similarity(token_vecs_sum[10].reshape(1,-1), token_vecs_sum[6].reshape(1,-1))[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z7XVD3P5A97O",
        "colab": {}
      },
      "source": [
        "print (\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'bank vault':\",  same_bank)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I_qFFA2IA97S",
        "colab": {}
      },
      "source": [
        "print (\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'river bank':\",  different_bank)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q674wxL94PvF",
        "colab_type": "text"
      },
      "source": [
        "#### <font color='red'>Try for yourself</font>\n",
        "\n",
        "Can you find one more example where the similarity of word embeddings for the same word in a similar context (such as 'bank' in 'bank robber' and 'bank' in 'bank vault') is greater than the similarity of word embeddings for same words in a different context (such as 'bank' in 'bank robber' and 'river bank')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qmz8eTK1A97Y"
      },
      "source": [
        "## II. Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_tdM1nNpA97b"
      },
      "source": [
        "Next we'll use the `bert-base-uncased` pre-trained model and test the embeddings on a classification task. For this task we'll be using a food reviews dataset and build a small classification model to classify reviews as either positive or negative. \n",
        "\n",
        "For this exercise we take a look at two ways of creating word embeddings:\n",
        "1. **Summing** together the hidden state of the last four layers. \n",
        "2. Using the hidden state of the last layer only.\n",
        "\n",
        "Both of these methods would create a word embedding of shape 768. One simple way of creating **sentence embeddings** is to extract word vectors for each of the tokens in a given sentence and calculate the mean vector of all the token embeddings. This is how we'll be creating our sentence embeddings for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg3_P58FSgjQ",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xFb6lDf0Rx5k",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "brgE_KeXvbKh",
        "colab": {}
      },
      "source": [
        "def show_confusion_matrix(C, class_labels=['0', '1']):\n",
        "    \"\"\"\n",
        "    :C: shape (2,2) as given by scikit-learn confusion_matrix function ['ndarray']\n",
        "    :class_labels: list of strings, default simply labels 0 and 1 ['List']\n",
        "\n",
        "    Draws confusion matrix with associated metrics.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    assert C.shape == (2, 2), \"Confusion matrix should be from binary classification only.\"\n",
        "\n",
        "    # true negative, false positive, etc...\n",
        "    tn = C[0, 0]\n",
        "    fp = C[0, 1]\n",
        "    fn = C[1, 0]\n",
        "    tp = C[1, 1]\n",
        "\n",
        "    NP = fn + tp  # Num positive examples\n",
        "    NN = tn + fp  # Num negative examples\n",
        "    N = NP + NN\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n",
        "\n",
        "    # Draw the grid boxes\n",
        "    ax.set_xlim(-0.5, 2.5)\n",
        "    ax.set_ylim(2.5, -0.5)\n",
        "    ax.plot([-0.5, 2.5], [0.5, 0.5], '-k', lw=2)\n",
        "    ax.plot([-0.5, 2.5], [1.5, 1.5], '-k', lw=2)\n",
        "    ax.plot([0.5, 0.5], [-0.5, 2.5], '-k', lw=2)\n",
        "    ax.plot([1.5, 1.5], [-0.5, 2.5], '-k', lw=2)\n",
        "\n",
        "    # Set xlabels\n",
        "    ax.set_xlabel('Predicted Label', fontsize=16)\n",
        "    ax.set_xticks([0, 1, 2])\n",
        "    ax.set_xticklabels(class_labels + [''])\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    ax.xaxis.tick_top()\n",
        "    # These coordinate might require some tinkering. Ditto for y, below.\n",
        "    ax.xaxis.set_label_coords(0.34, 1.06)\n",
        "\n",
        "    # Set ylabels\n",
        "    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n",
        "    ax.set_yticklabels(class_labels + [''], rotation=90)\n",
        "    ax.set_yticks([0, 1, 2])\n",
        "    ax.yaxis.set_label_coords(-0.09, 0.65)\n",
        "\n",
        "    # Fill in initial metrics: tp, tn, etc...\n",
        "    ax.text(0, 0,\n",
        "            'True Neg: %d\\n(Num Neg: %d)' % (tn, NN),\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    ax.text(0, 1,\n",
        "            'False Neg: %d' % fn,\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    ax.text(1, 0,\n",
        "            'False Pos: %d' % fp,\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    ax.text(1, 1,\n",
        "            'True Pos: %d\\n(Num Pos: %d)' % (tp, NP),\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    # Fill in secondary metrics: accuracy, true pos rate, etc...\n",
        "    ax.text(2, 0,\n",
        "            'False Pos Rate: %.2f' % (fp / (fp + tn + 0.)),\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    ax.text(2, 1,\n",
        "            'True Pos Rate: %.2f' % (tp / (tp + fn + 0.)),\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    ax.text(2, 2,\n",
        "            'Accuracy: %.2f' % ((tp + tn + 0.) / N),\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    ax.text(0, 2,\n",
        "            'Neg Pre Val: %.2f' % (1 - fn / (fn + tn + 0.)),\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    ax.text(1, 2,\n",
        "            'Pos Pred Val: %.2f' % (tp / (tp + fp + 0.)),\n",
        "            va='center',\n",
        "            ha='center',\n",
        "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4OFR1hAV99U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings:\n",
        "    LAST_LAYER = 1\n",
        "    LAST_4_LAYERS = 2\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self._bert_model.eval()\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: tokenized sentence based on word piece model ['List']\n",
        "        \"\"\"\n",
        "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
        "        return tokenized_text\n",
        "\n",
        "    def get_bert_embeddings(self, sentence):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
        "        \"\"\"\n",
        "        # Predict hidden states features for each layer\n",
        "\n",
        "        tokenized_text = self.tokenize(sentence)\n",
        "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self._bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "\n",
        "        return encoded_layers[-1][0:12]\n",
        "\n",
        "    def sentence2vec(self, sentence, layers):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence: input sentence ['str']\n",
        "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
        "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
        "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
        "\n",
        "        :return: sentence vector [List]\n",
        "        \"\"\"\n",
        "        encoded_layers = self.get_bert_embeddings(sentence)\n",
        "        \n",
        "        if layers == 1:\n",
        "            # using the last layer embeddings\n",
        "            token_embeddings = encoded_layers[-1]\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
        "            return sentence_embedding.view(-1).tolist()\n",
        "\n",
        "        elif layers == 2:\n",
        "            token_embeddings = []\n",
        "            tokenized_text = self.tokenize(sentence)\n",
        "\n",
        "            batch_i = 0\n",
        "            # For each token in the sentence...\n",
        "            for token_i in range(len(tokenized_text)):\n",
        "\n",
        "                # Holds 12 layers of hidden states for each token\n",
        "                hidden_layers = []\n",
        "\n",
        "                # For each of the 12 layers...\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    # Lookup the vector for `token_i` in `layer_i`\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "                    hidden_layers.append(list(vec.numpy()))\n",
        "\n",
        "                token_embeddings.append(hidden_layers)\n",
        "\n",
        "            # using the last 4 layer embeddings\n",
        "            token_vecs_sum = []\n",
        "\n",
        "            # For each token in the sentence...\n",
        "            for token in token_embeddings:\n",
        "                # Sum the vectors from the last four layers.\n",
        "                sum_vec = np.sum(token[-4:], axis=0)\n",
        "\n",
        "                # Use `sum_vec` to represent `token`.\n",
        "                token_vecs_sum.append(list(sum_vec))\n",
        "\n",
        "            # summing the last layer vectors for each token\n",
        "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
        "            return sentence_embedding.ravel().tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SEbLFCReA97h",
        "colab": {}
      },
      "source": [
        "def model_grid_search(parameters, x_train, y_train, model, scoring):\n",
        "    \"\"\"\n",
        "\n",
        "    :param parameters: hyperparameter choices to tune. Eg:\n",
        "                    Hyperparameter values for logistic regression\n",
        "                    {'loss' :['log'],\n",
        "                    'penalty':['l1','l2','elasticnet'],\n",
        "                    'alpha':[float(i)/10000000 for i in range(1,5,1)],\n",
        "                    'n_jobs':[-1]}\n",
        "\n",
        "                    [dict]\n",
        "\n",
        "    :param x_train: training data (2D matrix) [List or ndarray]\n",
        "    :param y_train: training data predictor values [List or ndarray]\n",
        "    :param model: sklearn model to perform gridsearch on\n",
        "    :param scoring: Finds the best model based on the scoring metric used (https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
        "    :return: grid search instance with information on all trained classifiers\n",
        "    \"\"\"\n",
        "    clf = GridSearchCV(model, parameters, cv=5, scoring=scoring)\n",
        "    clf.fit(x_train, y_train)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def test_metrics(y_pred, y_test):\n",
        "    \"\"\"\n",
        "    \n",
        "    :param y_pred: predicted values [List]\n",
        "    :param y_test: actual predictor values [List]\n",
        "    \n",
        "    :return precision, recall, F1 and confusion matrix scores [Dict]\n",
        "    \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Precision and recall\n",
        "    fp = conf_matrix[0, 1]\n",
        "    fn = conf_matrix[1, 0]\n",
        "    tp = conf_matrix[1, 1]\n",
        "\n",
        "    precision = 100 * float(tp) / (tp + fp)\n",
        "    recall = 100 * float(tp) / (tp + fn)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    \n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'F1': F1,\n",
        "        'confusion_matrix': conf_matrix\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oKpuRelAA97m",
        "colab": {}
      },
      "source": [
        "embed_model = Embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEE6jwRfSgjj",
        "colab_type": "text"
      },
      "source": [
        "#### Dataset\n",
        "This dataset consists of reviews of fine foods from Amazon. The original dataset spans a period of more than 10 years, including ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories. The `Score` column consists of scores ranging from 1 to 5 where **1** and **2** represent negative reviews, **4** and **5** represent positive reviews and a score of **3** mean the review is neutral. For simplicity we've combined scores **4,5** and tagged it as a positive score and combined scores **1,2** and tagged it as a negative score. Reviews with a score of **3** (neutal review) have not beeen incuded. The `Text` and `Summary` columns have been pre-processed. <a href=\"https://www.kaggle.com/snap/amazon-fine-food-reviews\">Here</a> is a link to the original dataset.\n",
        "\n",
        "We have provided a dataset with pre-preprocessed text and summary column. To obtain the sentence vector for our model, we'll be focusing on just the `Text` column. In the `Score` column a score of 1 indicates a positive review and a score of 0 indicates a negative review. The dataset is highly imbalanced with ~88 % of reviews belonging to the positive class (representend with a 1 in the `Score` column) and ~12 % belonging to the negative class (represented with a 0 in the `Score` column).\n",
        "\n",
        "Run the below two cells to download the pre-processed train and test csv files. The dataset has been hosted on Google Drive and has been made available to anyone with the link. \n",
        "\n",
        "1. https://medium.com/@annissouames99/how-to-upload-files-automatically-to-drive-with-python-ee19bb13dda\n",
        "2. Link to `train.csv` : https://drive.google.com/open?id=1IyCOmYJFILPkhx6ASvI0iJ_3Obvw1g0O\n",
        "3. Link to `test.csv` : https://drive.google.com/open?id=1g0RoPXFUUgUGh4Mxa3QY_J_tz3LO_IqA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEZOXw2ppmEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6F5-35fuDDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_train = \"1IyCOmYJFILPkhx6ASvI0iJ_3Obvw1g0O\"\n",
        "id_test = \"1g0RoPXFUUgUGh4Mxa3QY_J_tz3LO_IqA\"\n",
        "\n",
        "df_train_downloaded = drive.CreateFile({'id':id_train}) \n",
        "df_train_downloaded.GetContentFile('train.csv')\n",
        "\n",
        "df_test_downloaded = drive.CreateFile({'id':id_test}) \n",
        "df_test_downloaded.GetContentFile('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNbFO_PwvV2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should have train.csv and test.csv\n",
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxEYDnB5Sgjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZFsxePaCA97x",
        "colab": {}
      },
      "source": [
        "df_train.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01WhECmySgjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i7B_E8e2A98D"
      },
      "source": [
        "### Modeling\n",
        "\n",
        "We will be using logistic regression to build a baseline model. Logistic regression is a linear model for classification and a good option for building our baseline. We'll be using sklearn module to build our model. We also use the grid search method to find the best hyperparameters for the logistic regression model and observe the model performance. <a href=\"https://ayearofai.com/rohan-6-follow-up-statistical-interpretation-of-logistic-regression-e78de3b4d938\">Here</a> is a small refresher on logistic regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbDJc69gSgjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data_size = 5000  # you can experiment with more data to get a more realistic performance score. With a fewer datapoints the model tends to overfit\n",
        "\n",
        "text_train = df_train['Text'].iloc[0:training_data_size].values\n",
        "y_train = df_train['Score'].iloc[0:training_data_size].values\n",
        "\n",
        "text_test = df_train['Text'].iloc[0:training_data_size].values\n",
        "y_test = df_train['Score'].iloc[0:training_data_size].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pAcqzVTSA98N"
      },
      "source": [
        "#### 1. BERT last layer hidden state as embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "y-lnbua8SgkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = []\n",
        "for sentence in tqdm(text_train):\n",
        "    x_train.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_LAYER))\n",
        "    \n",
        "x_test = []\n",
        "for sentence in tqdm(text_test):\n",
        "    x_test.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_LAYER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D1Z9y6pSgkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'loss' :['log'],'penalty':['l1','l2','elasticnet'],'alpha':[float(i)/10000000 for i in range(1,5,1)],'n_jobs':[-1]}\n",
        "LR = SGDClassifier(fit_intercept=True, random_state=42)\n",
        "scoring = 'recall'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sIcu3EMPA98Q",
        "colab": {}
      },
      "source": [
        "clf = model_grid_search(parameters, x_train, y_train, model=LR, scoring=scoring)\n",
        "print(\"Best params grid search\", clf.best_params_)\n",
        "y_pred = clf.predict(x_test)\n",
        "metrics_dict = test_metrics(y_pred, y_test)\n",
        "print(\"Precision\", metrics_dict['precision'])\n",
        "print(\"Recall\", metrics_dict['recall'])\n",
        "print(\"f1 score\", metrics_dict['F1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qoVl36GYA98U",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "show_confusion_matrix(metrics_dict['confusion_matrix'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXGvw1viSgj1",
        "colab_type": "text"
      },
      "source": [
        "#### 2.  BERT last 4 layer hidden states as embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nf9d4Uf2A98E",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "x_train = []\n",
        "for sentence in tqdm(text_train):\n",
        "    x_train.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_4_LAYERS))\n",
        "    \n",
        "x_test = []\n",
        "for sentence in tqdm(text_test):\n",
        "    x_test.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_4_LAYERS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOeg2ZjiSgj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'loss' :['log'],'penalty':['l1','l2','elasticnet'],'alpha':[float(i)/10000000 for i in range(1,5,1)],'n_jobs':[-1]}\n",
        "LR = SGDClassifier(fit_intercept=True, random_state=42)\n",
        "scoring = 'recall'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XhI5OaeUA98G",
        "colab": {}
      },
      "source": [
        "LR = SGDClassifier(fit_intercept=True)\n",
        "clf = model_grid_search(parameters, x_train, y_train, model=LR, scoring=scoring)\n",
        "print(\"Best params grid search\", clf.best_params_)\n",
        "y_pred = clf.predict(x_test)\n",
        "metrics_dict = test_metrics(y_pred, y_test)\n",
        "print(\"Precision\", metrics_dict['precision'])\n",
        "print(\"Recall\", metrics_dict['recall'])\n",
        "print(\"f1 score\", metrics_dict['F1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei1PsG2niiy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_confusion_matrix(metrics_dict['confusion_matrix'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZaMzno1SgkY",
        "colab_type": "text"
      },
      "source": [
        "## Things to try:\n",
        "\n",
        "1. Try different non-linear classifiers (decision tree, random forest .etc) to achieve better results than the baseline.\n",
        "2. Try different ways of building your word embeddings and use that on your desired classification model: (refer to the previous section [above](https://colab.research.google.com/drive/1g7OWTJKVWwij9Bit9nZDamaUI2--fjmO#scrollTo=ytonVXQWGcei))\n",
        "    - Sum of all 12 layer hidden states (word embedding would be of size 768)\n",
        "    - First layer hidden state (word embedding would be of size 768)\n",
        "    - Second to last layer hidden state (word embedding would be of size 768)\n",
        "    - Concat of last 4 layer hidden states (word embedding would be of size 768 * 4)\n",
        "3. Use larger models such as `bert-large-uncased` instead of `bert-base-uncased`\n",
        "3. You can also use `BERT as service`, to obtain <a href=\"https://github.com/hanxiao/bert-as-service#getting-elmo-like-contextual-word-embedding\">sentence embeddings</a> (it supports many other features as well and their repo is pretty well documented) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hnay4sEA98Y"
      },
      "source": [
        "## References:\n",
        "1. https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
        "2. http://jalammar.github.io/illustrated-bert/\n",
        "3. https://jalammar.github.io/illustrated-transformer/\n",
        "4. https://towardsdatascience.com/nlp-extract-contextualized-embeddings-from-bert-keras-tf-67ef29f60a7b\n",
        "5. [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html): a blogpost by Professor Sasha Rush describing the transformer architecture by implementing it from the paper in PyTorch."
      ]
    }
  ]
}